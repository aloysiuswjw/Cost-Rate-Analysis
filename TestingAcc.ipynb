{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from sklearn.model_selection import KFold\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Importing Old Dataset</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset and clean, ready as a dataframe for creating keys\n",
    "def createDF(datasets):\n",
    "    df = pd.read_csv(datasets, converters={'PARTY_ID': str, 'COM_ID': str, 'CNTR_SIZE': str})\n",
    "\n",
    "    # Formating to type and remove NaN values\n",
    "    df['POD'] = pd.to_datetime(df['POD'])\n",
    "    df['ENCODED_TYPE'] = df['ENCODED_TYPE'].fillna(-1).astype(int)\n",
    "    df = df.dropna(subset=['ENCODED_TYPE'])\n",
    "    df['RATE'] = df['RATE'].fillna(-1).astype(float)\n",
    "    df = df.dropna(subset=['RATE'])\n",
    "    df['ENCODED_TYPE'] = df['ENCODED_TYPE'].astype(int)\n",
    "    df_clean= df.dropna().reset_index(drop=True)\n",
    "\n",
    "    # Selecting and rearranging columns\n",
    "    sel_col = ['CSL_ID', 'CNTR_ID','POD_ID','ETD_POL_D','PARTY_ID',\n",
    "            'PARTY_NAME','POD','CNTR_SIZE','CNTR_TYPE','RATE']\n",
    "    df_fc = df_clean[sel_col]\n",
    "\n",
    "    # Removing years we do not want to process in our models\n",
    "    df_filtered = df_fc[df_fc['POD'].dt.year != 2002]\n",
    "\n",
    "    # Sorting the dates\n",
    "    df_filtered = df_filtered.sort_values(by='POD').reset_index(drop=True)\n",
    "    \n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataframes\n",
    "old_data = '.\\Datasets\\CR_COST_FC.csv'\n",
    "df1 = createDF(old_data)\n",
    "df1.head()\n",
    "df1.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = '.\\Datasets\\CR_COST_FC_new.csv'\n",
    "df2 = createDF(new_data)\n",
    "df2.head()\n",
    "df2.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Creating Dictionary Keys</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataframe(df):\n",
    "    filtered_dataframes = {}\n",
    "\n",
    "    for (port, size, ctype, party_id), group in df.groupby(['POD_ID', 'CNTR_SIZE', 'CNTR_TYPE', 'PARTY_ID']):\n",
    "        group = group.reset_index(drop=True).sort_values(by='POD')\n",
    "        df_id = f\"Port_{port}_Size_{size}_Type_{ctype}_PartyID_{party_id}\"\n",
    "        filtered_dataframes[df_id] = group\n",
    "\n",
    "    return filtered_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataframe1 = filter_dataframe(df1)\n",
    "df_ids1 = list(filtered_dataframe1.keys())\n",
    "print(list(df_ids1))\n",
    "print(len(list(df_ids1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataframe2 = filter_dataframe(df2)\n",
    "df_ids2 = list(filtered_dataframe2.keys())\n",
    "print(list(df_ids2))\n",
    "print(len(list(df_ids2)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Getting Top 5 ports keys</h4>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting only the top 5 ports as there are substantial data for training, testing and forecasting. I named it as keybunch as there are many keys for one dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTop5Ports(keybunch):\n",
    "    keybunch_pouch = []\n",
    "    \n",
    "    # Get a dictionary with key and number of rows for each dataframe in filtered_dataframes\n",
    "    key_row_counts = {key: len(keybunch[key]) for key in keybunch}\n",
    "\n",
    "    # Sort the key_row_counts dictionary by value (number of rows) in descending order\n",
    "    sorted_key_row_counts = sorted(key_row_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "    # Get the top 5 keys with the most rows\n",
    "    top_5_keys_tuple = sorted_key_row_counts[:5]\n",
    "\n",
    "    # Create a dictionary with the top 5 keys and their corresponding dataframes (with up to 5 rows per dataframe)\n",
    "    keybunch_subset = {}\n",
    "\n",
    "    for key, row_count in top_5_keys_tuple:\n",
    "        keybunch_subset[key] = keybunch[key][:5]\n",
    "        print(f\"Number of rows in {key}: {row_count}\")\n",
    "        keybunch_pouch.append(key)\n",
    "    \n",
    "    # Return array of keys\n",
    "    return keybunch_pouch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Old Dataset Keybunch:')\n",
    "old_df = getTop5Ports(filtered_dataframe1)\n",
    "print('\\n')\n",
    "\n",
    "print('New Dataset Keybunch:')\n",
    "new_df = getTop5Ports(filtered_dataframe2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Working with one, the highest count</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing the highest count in the each keypouch, new and old.\n",
    "sel_df = filtered_dataframe1[old_df[0]]\n",
    "sel_df.tail(5)\n",
    "\n",
    "latest_sel_df = filtered_dataframe2[new_df[0]]\n",
    "latest_sel_df.tail(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Latest datapoints from Latest dataframe.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the maximum date in the old dataframe\n",
    "max_date_in_old = sel_df['POD'].max()\n",
    "\n",
    "# Create a new dataframe that only includes rows from the latest dataframe where the date is greater than the maximum date in the old dataframe\n",
    "new_dates_df = latest_sel_df[latest_sel_df['POD'] > max_date_in_old].reset_index(drop=True)\n",
    "\n",
    "# Print the new dataframe\n",
    "new_dates_df.head()\n",
    "new_dates_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Start to prepare dataframe for LSTM and ARIMA<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = sel_df.groupby(['POD', 'RATE']).size().reset_index(name='Count')\n",
    "sorted_df = grouped_df.sort_values(by='Count', ascending=False)\n",
    "sorted_df.head(10)\n",
    "sorted_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated dates and cost rows\n",
    "sel_df = sel_df.drop_duplicates(subset=['POD', 'RATE']).reset_index(drop=True)\n",
    "\n",
    "# Remove features that are not important\n",
    "sel_feat = ['POD','RATE']\n",
    "sel_df = sel_df[sel_feat]\n",
    "\n",
    "sel_df.head(10)\n",
    "sel_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate using spline interpolation\n",
    "\n",
    "# Create a new dataframe with a date range from min to max date in your dataframe\n",
    "new_df = pd.DataFrame()\n",
    "new_df['POD'] = pd.date_range(start=sel_df['POD'].min(), end=sel_df['POD'].max())\n",
    "\n",
    "# Merge the original dataframe with the new one. Missing dates in the original dataframe will be filled with NaN\n",
    "df_interpolated = pd.merge(new_df, sel_df, on='POD', how='left')\n",
    "\n",
    "# Perform spline interpolation\n",
    "df_interpolated['RATE'] = df_interpolated['RATE'].interpolate(method='spline', order=2)\n",
    "\n",
    "df_interpolated.head(10)\n",
    "df_interpolated.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(df_interpolated['POD'], df_interpolated['RATE'], color='red', label=\"Interpolated Data\")\n",
    "plt.plot(sel_df['POD'], sel_df['RATE'], color='blue', label=\"Actual Data\")\n",
    "\n",
    "plt.xlabel('Date(Days)')\n",
    "plt.ylabel('Cost Rate(USD)')\n",
    "plt.title(old_df[0])\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Mean Square Error Evaluation Function</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Square Error Function:\n",
    "def calculate_RMSE(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>LSTM Regression<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restructure time series data for LSTM model\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - look_back - 1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "def create_LSTM_model(trainX, trainY, testX, testY):\n",
    "    # LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(4, input_shape=(trainX.shape[1], trainX.shape[2]), activation='tanh', recurrent_activation='hard_sigmoid'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    # Fit the model and store the history\n",
    "    history = model.fit(trainX, trainY, epochs=300, batch_size=1, verbose=2, validation_data=(testX, testY))\n",
    "    return model, history\n",
    "\n",
    "# To see if overfitting, underfitting or good fit\n",
    "def plot_train_val_loss(history):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model loss progress during training and validation')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot Prediction Function to see model fitting\n",
    "def plot_predictions_v1(df, trainPredict, testPredict, look_back):\n",
    "    \n",
    "    # shift train predictions for plotting\n",
    "    trainPredictPlot = np.empty_like(df)\n",
    "    trainPredictPlot[:, :] = np.nan\n",
    "    trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "\n",
    "    # shift test predictions for plotting\n",
    "    testPredictPlot = np.empty_like(df)\n",
    "    testPredictPlot[:, :] = np.nan\n",
    "    print(f'testPredict shape: {testPredict.shape}')\n",
    "    print(f'testPredictPlot section shape: {testPredictPlot[len(trainPredict)+(look_back*2)+1:len(df)-1, :].shape}')\n",
    "    testPredictPlot[len(trainPredict)+(look_back*2)+1:len(df)-1, :1] = testPredict\n",
    "\n",
    "    # plot baseline and predictions\n",
    "    plt.figure(figsize=(15,8))\n",
    "    plt.plot(scaler.inverse_transform(df), color='blue', label='Actual Data')\n",
    "    plt.plot(trainPredictPlot, color='orange', label='Training Fit')\n",
    "    plt.plot(testPredictPlot, color='green', label='Testing Prediction')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize dataset for LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(df_interpolated['RATE'].values.reshape(-1,1))\n",
    "\n",
    "# Split into train and test sets\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "\n",
    "# Reshape into X=t and Y=t+1, timestep  look_back\n",
    "look_back = 1\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "# Reshape input to be [samples, time steps, features]\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n",
    "# Store the model\n",
    "model = create_LSTM_model(trainX, trainY, testX, testY)\n",
    "\n",
    "# Evalute LSTM Model\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "\n",
    "# Invert predictions back to prescaled values\n",
    "# This is to compare with original input\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "\n",
    "# Calculate mean squared error\n",
    "trainScore = calculate_RMSE(trainY[0], trainPredict[:,0])\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = calculate_RMSE(testY[0], testPredict[:,0])\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "# # View how LSTM model fitting to the actual data\n",
    "# plot_predictions_v1(dataset, trainPredict, testPredict, look_back)\n",
    "\n",
    "# Store the model and its history\n",
    "model, history = create_LSTM_model(trainX, trainY, testX, testY)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plot_train_val_loss(history)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
