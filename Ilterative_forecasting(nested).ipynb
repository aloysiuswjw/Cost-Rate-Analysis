{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Iterative Nested Forecasting</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "# For machine learning\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from math import sqrt\n",
    "from pmdarima import auto_arima\n",
    "from prophet import Prophet\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "pd.options.display.max_rows = 100\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Importing Datasets</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset and clean, ready as a dataframe for creating keys\n",
    "def createDF(datasets):\n",
    "    df = pd.read_csv(datasets, converters={\n",
    "                     'PARTY_ID': str, 'COM_ID': str, 'CNTR_SIZE': str})\n",
    "\n",
    "    # Formating to type and remove NaN values\n",
    "    df['POD'] = pd.to_datetime(df['POD'])\n",
    "    df['ENCODED_TYPE'] = df['ENCODED_TYPE'].fillna(-1).astype(int)\n",
    "    df = df.dropna(subset=['ENCODED_TYPE'])\n",
    "    df['RATE'] = df['RATE'].fillna(-1).astype(float)\n",
    "    df = df.dropna(subset=['RATE'])\n",
    "    df['ENCODED_TYPE'] = df['ENCODED_TYPE'].astype(int)\n",
    "    df_clean = df.dropna().reset_index(drop=True)\n",
    "\n",
    "    # Selecting and rearranging columns\n",
    "    sel_col = ['CSL_ID', 'CNTR_ID', 'POD_ID', 'ETD_POL_D', 'PARTY_ID',\n",
    "               'PARTY_NAME', 'POD', 'CNTR_SIZE', 'CNTR_TYPE', 'RATE']\n",
    "    df_fc = df_clean[sel_col]\n",
    "\n",
    "    # Removing years we do not want to process in our models\n",
    "    df_filtered = df_fc[df_fc['POD'].dt.year != 2002]\n",
    "\n",
    "    # Sorting the dates\n",
    "    df_filtered = df_filtered.sort_values(by='POD').reset_index(drop=True)\n",
    "\n",
    "    return df_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataframes for old and new\n",
    "old_data = '.\\Datasets\\CR_COST_FC.csv'\n",
    "df1 = createDF(old_data)\n",
    "df1.head()\n",
    "\n",
    "new_data = '.\\Datasets\\CR_COST_FC_new.csv'\n",
    "df2 = createDF(new_data)\n",
    "df2.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Creating Dictionary Keys</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is to filter and create keys\n",
    "def filter_dataframe(df):\n",
    "    filtered_dataframes = {}\n",
    "\n",
    "    for (port, size, ctype, party_id), group in df.groupby(['POD_ID', 'CNTR_SIZE', 'CNTR_TYPE', 'PARTY_ID']):\n",
    "        group = group.reset_index(drop=True).sort_values(by='POD')\n",
    "        df_id = f\"Port_{port}_Size_{size}_Type_{ctype}_PartyID_{party_id}\"\n",
    "        filtered_dataframes[df_id] = group\n",
    "\n",
    "    return filtered_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating keys from data\n",
    "print(\"Old Data keys:\")\n",
    "filtered_dataframe1 = filter_dataframe(df1)\n",
    "df_ids1 = list(filtered_dataframe1.keys())\n",
    "print(list(df_ids1))\n",
    "print(len(list(df_ids1)))\n",
    "\n",
    "print(\"\\nNew Data keys:\")\n",
    "filtered_dataframe2 = filter_dataframe(df2)\n",
    "df_ids2 = list(filtered_dataframe2.keys())\n",
    "print(list(df_ids2))\n",
    "print(len(list(df_ids2)))\n",
    "\n",
    "# Removing Keys that have less then 500 rows as it is not enough data points for LSTM\n",
    "print(\"\\nRemoving keys that has less then 500 entries:\")\n",
    "# Old data keys\n",
    "filtered_dataframe1_large = {key: df for key, df in filtered_dataframe1.items() if len(df) >= 1000}\n",
    "large_df_ids1 = list(filtered_dataframe1_large.keys())\n",
    "print(list(large_df_ids1))\n",
    "print(len(list(large_df_ids1)))\n",
    "print(\"\\n\")\n",
    "\n",
    "# New data keys\n",
    "filtered_dataframe2_large = {key: df for key, df in filtered_dataframe2.items() if len(df) >= 1000}\n",
    "large_df_ids2 = list(filtered_dataframe2_large.keys())\n",
    "print(list(large_df_ids2))\n",
    "print(len(list(large_df_ids2)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Preprocessing</h2>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Interpolate old_df missing values and group missing entries by weeks</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_and_aggregate(df, key):\n",
    "    \n",
    "    # Initialise RobustScaler(handle outliers)\n",
    "    scaler = RobustScaler()\n",
    "    df[['RATE']] = scaler.fit_transform(df[['RATE']])\n",
    "    \n",
    "    # Check if the DataFrame has any NaN values\n",
    "    if df.isna().any().any():\n",
    "        print(f\"{key} DataFrame contains NaN values.\")\n",
    "    else:\n",
    "        print(f\"{key} DataFrame does not contain NaN values.\")\n",
    "    \n",
    "    # Drop duplicates\n",
    "    sel_df = df.drop_duplicates(subset=['POD', 'RATE']).reset_index(drop=True)\n",
    "\n",
    "    # Extract POD_ID and PARTY_ID from the first row\n",
    "    pod_id = df['POD_ID'].iloc[0]\n",
    "    party_id = df['PARTY_ID'].iloc[0]\n",
    "\n",
    "    # Create a new dataframe with a date range from min to max date in your dataframe\n",
    "    new_df = pd.DataFrame()\n",
    "    new_df['POD'] = pd.date_range(\n",
    "        start=sel_df['POD'].min(), end=sel_df['POD'].max())\n",
    "\n",
    "    # Merge the original dataframe with the new one\n",
    "    df_interpolated = pd.merge(\n",
    "        new_df, sel_df[['POD', 'RATE']], on='POD', how='left')\n",
    "\n",
    "    # Perform spline interpolation\n",
    "    df_interpolated['RATE'] = df_interpolated['RATE'].interpolate(\n",
    "        method='polynomial', order=1)\n",
    "    df_interpolated['RATE'] = df_interpolated['RATE'].round(2)\n",
    "\n",
    "    # Create YearMonthWeek directly from the 'POD'\n",
    "    df_interpolated['YearMonthWeek'] = df_interpolated['POD'] - \\\n",
    "        pd.to_timedelta(df_interpolated['POD'].dt.dayofweek, unit='D')\n",
    "\n",
    "    # Create a new dataframe with every week in the range\n",
    "    all_weeks = pd.date_range(start=df_interpolated['POD'].min(\n",
    "    ), end=df_interpolated['POD'].max(), freq='W')\n",
    "    all_weeks_df = pd.DataFrame(all_weeks, columns=['POD'])\n",
    "\n",
    "    # Create YearMonthWeek in all_weeks_df\n",
    "    all_weeks_df['YearMonthWeek'] = all_weeks_df['POD'] - \\\n",
    "        pd.to_timedelta(all_weeks_df['POD'].dt.dayofweek, unit='D')\n",
    "\n",
    "    # Merge this with your original dataframe\n",
    "    merged_df = pd.merge(all_weeks_df, df_interpolated,\n",
    "                         on=['YearMonthWeek'], how='left')\n",
    "\n",
    "    # Group by YearMonthWeek and compute your rate\n",
    "    grouped = merged_df.groupby(['YearMonthWeek'])\n",
    "\n",
    "    agg_df = pd.DataFrame(\n",
    "        columns=['YearMonthWeek', 'Rate', 'POD_ID', 'PARTY_ID'])\n",
    "\n",
    "    for group_name, group_df in grouped:\n",
    "        year_month_week = group_name\n",
    "\n",
    "        # Calculate skewness of RATE values\n",
    "        rate_skew = group_df['RATE'].skew()\n",
    "\n",
    "        # Calculate trimmed mean of RATE values\n",
    "        # trimming 10% from each end\n",
    "        rate_metric = stats.trim_mean(group_df['RATE'].dropna().values, 0.1)\n",
    "\n",
    "        new_row = {'YearMonthWeek': year_month_week,\n",
    "                   'Rate': rate_metric, 'POD_ID': pod_id, 'PARTY_ID': party_id}\n",
    "\n",
    "        # Append row to aggregated dataframe\n",
    "        agg_df = agg_df.append(new_row, ignore_index=True)\n",
    "\n",
    "    agg_df = agg_df.sort_values(by='YearMonthWeek').reset_index(drop=True)\n",
    "\n",
    "    return agg_df\n",
    "\n",
    "\n",
    "# dictionary to store the results\n",
    "processed_dfs = {}\n",
    "\n",
    "# loop over all keys in the original dictionary\n",
    "for key in filtered_dataframe1_large.keys():\n",
    "    processed_dfs[key] = interpolate_and_aggregate(\n",
    "        filtered_dataframe1_large[key], key)\n",
    "\n",
    "# Preview dictionary\n",
    "print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Sorting and getting key arrays</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPortKeys(keybunch):\n",
    "    keybunch_pouch = []\n",
    "    # Create a dictionary with corresponding dataframes\n",
    "    keybunch_subset = {}\n",
    "    \n",
    "    # Get a dictionary with key and number of rows for each dataframe in filtered_dataframes\n",
    "    key_row_counts = {key: len(keybunch[key]) for key in keybunch}\n",
    "\n",
    "    # Sort the key_row_counts dictionary by value (number of rows) in descending order\n",
    "    sorted_key_row_counts = sorted(\n",
    "        key_row_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "    for key, row_count in sorted_key_row_counts:\n",
    "            keybunch_subset[key] = keybunch[key]\n",
    "            print(f\"Number of rows in {key}: {row_count}\")\n",
    "            keybunch_pouch.append(key)\n",
    "\n",
    "    # Return array of keys\n",
    "    return keybunch_pouch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is changing it to an array\n",
    "\n",
    "print('Processed Old Dataset Keybunch:')\n",
    "process_old_df= getPortKeys(processed_dfs)\n",
    "print(len(process_old_df))\n",
    "print('\\n')\n",
    "\n",
    "print('Old Dataset Keybunch:')\n",
    "old_df= getPortKeys(filtered_dataframe1_large)\n",
    "print(len(old_df))\n",
    "print('\\n')\n",
    "\n",
    "print('New Dataset Keybunch:')\n",
    "new_df= getPortKeys(filtered_dataframe2_large)\n",
    "print(len(new_df))\n",
    "print('\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Iterative Nested Forecasting</h2>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Time series pipeline for best forecasting model to each dataframe.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lagged_features(df, lags):\n",
    "    df = df.copy()\n",
    "    for i in range(1, lags + 1):\n",
    "        df[f'Rate_lag_{i}'] = df['Rate'].shift(i)\n",
    "    return df\n",
    "\n",
    "\n",
    "def train_auto_arima(train, test):\n",
    "    model = auto_arima(train['Rate'], seasonal=True, trace=True,\n",
    "                       error_action='ignore', suppress_warnings=True)\n",
    "    model.fit(train['Rate'])\n",
    "    forecast, conf_int = model.predict(\n",
    "        n_periods=len(test), return_conf_int=True)\n",
    "    pred_test = pd.Series(forecast, index=test.index)\n",
    "    return pred_test\n",
    "\n",
    "\n",
    "def train_prophet(train, test):\n",
    "    train = train.rename(columns={'YearMonthWeek': 'ds', 'Rate': 'y'})\n",
    "    test = test.rename(columns={'YearMonthWeek': 'ds', 'Rate': 'y'})\n",
    "    model = Prophet()\n",
    "    model.fit(train)\n",
    "    forecast = model.predict(test)\n",
    "    pred_test = forecast['yhat']\n",
    "    return pred_test\n",
    "\n",
    "\n",
    "def train_tree_based_model(model_class, train, test, lags):\n",
    "    model = model_class(random_state=42)\n",
    "\n",
    "    # Construct a pipeline that performs imputation and then trains the model\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('imputation', SimpleImputer()),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    features = [f'Rate_lag_{i}' for i in range(1, lags+1)]\n",
    "    pipeline.fit(train[features], train['Rate'])\n",
    "    pred_test = pipeline.predict(test[features])\n",
    "\n",
    "    return pred_test\n",
    "\n",
    "\n",
    "def train_ets(train, test):\n",
    "    model = ExponentialSmoothing(endog=train['Rate'])\n",
    "    fit = model.fit()\n",
    "    pred = fit.predict(start=len(train), end=len(train) + len(test) - 1)\n",
    "    return pred\n",
    "\n",
    "\n",
    "lags = 1\n",
    "\n",
    "models = [\n",
    "    {\"name\": \"auto_arima\", \"train_function\": train_auto_arima},\n",
    "    {\"name\": \"prophet\", \"train_function\": train_prophet},\n",
    "    {\"name\": \"ets\", \"train_function\": train_ets},\n",
    "    {\"name\": \"RandomForest\", \"train_function\": train_tree_based_model,\n",
    "        \"model\": RandomForestRegressor},\n",
    "    {\"name\": \"SVM\", \"train_function\": train_tree_based_model, \"model\": SVR},\n",
    "    {\"name\": \"XGBoost\", \"train_function\": train_tree_based_model, \"model\": XGBRegressor}\n",
    "]\n",
    "\n",
    "for key in processed_dfs.keys():\n",
    "    df = processed_dfs[key]\n",
    "\n",
    "    # Create lagged features\n",
    "    df = create_lagged_features(df, lags)\n",
    "\n",
    "    # Drop any rows with NaN values in the original Rate column\n",
    "    df = df.dropna(subset=['Rate'])\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train_size = int(len(df) * 0.8)\n",
    "    train, test = df[:train_size], df[train_size:]\n",
    "\n",
    "    # Then, within your main loop, you can call the appropriate function:\n",
    "    for model_info in models:\n",
    "        try:\n",
    "            model_name = model_info[\"name\"]\n",
    "            train_function = model_info[\"train_function\"]\n",
    "            if \"model\" in model_info:\n",
    "                model = model_info[\"model\"]\n",
    "                pred_test = train_function(model, train, test, lags)\n",
    "            else:\n",
    "                pred_test = train_function(train, test)\n",
    "\n",
    "            test_rmse = sqrt(mean_squared_error(test['Rate'], pred_test))\n",
    "            print(f\"Model: {model_name}, Key: {key}, Test RMSE: {test_rmse}\")\n",
    "            model_results.append({\"model\": model_name, \"test_rmse\": test_rmse})\n",
    "\n",
    "        except Exception as e:\n",
    "            error_message = f\"Model: {model_name}, Key: {key} failed due to {str(e)}\"\n",
    "            print(error_message)\n",
    "            errors.append(error_message)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Forecasting with best model</h2>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Selecting Dataset(country)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable selector\n",
    "sel_country = old_df[1]\n",
    "print(sel_country)\n",
    "\n",
    "# Getting the latest data from new vs old as accuracy measure\n",
    "sel_process_old_df = processed_dfs[sel_country]\n",
    "sel_process_old_df.head(3)\n",
    "sel_process_old_df.info()\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "sel_old_df = filtered_dataframe1_large[sel_country]\n",
    "sel_old_df.head(3)\n",
    "sel_old_df.info()\n",
    "print(\"\\n\")\n",
    "\n",
    "sel_new_df = filtered_dataframe2_large[sel_country]\n",
    "sel_new_df.head(3)\n",
    "sel_new_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Forecasting</h4>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
