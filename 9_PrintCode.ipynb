{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>This is used to take picturs of code for report</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import skew\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "# ARIMA\n",
    "import pmdarima as pm\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "pd.options.display.max_rows = 100\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Importing Datasets</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CSL_ID</th>\n",
       "      <th>CNTR_ID</th>\n",
       "      <th>POD_ID</th>\n",
       "      <th>ETD_POL_D</th>\n",
       "      <th>PARTY_ID</th>\n",
       "      <th>PARTY_NAME</th>\n",
       "      <th>POD</th>\n",
       "      <th>CNTR_SIZE</th>\n",
       "      <th>CNTR_TYPE</th>\n",
       "      <th>RATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ECS01050600224</td>\n",
       "      <td>CTNR010050700354</td>\n",
       "      <td>ROTTERDAM</td>\n",
       "      <td>NLRTM</td>\n",
       "      <td>01008827</td>\n",
       "      <td>YANG MING (SINGAPORE) PTE. LTD.</td>\n",
       "      <td>2005-07-15</td>\n",
       "      <td>40</td>\n",
       "      <td>HC NOR</td>\n",
       "      <td>1620.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ECS01050600610</td>\n",
       "      <td>CTNR010050700353</td>\n",
       "      <td>FELIXSTOWE</td>\n",
       "      <td>GBFXT</td>\n",
       "      <td>01002303</td>\n",
       "      <td>GLOBELINK FALLOW LIMITED</td>\n",
       "      <td>2005-07-15</td>\n",
       "      <td>40</td>\n",
       "      <td>GP</td>\n",
       "      <td>1800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ECS01050600041</td>\n",
       "      <td>CTNR010050700351</td>\n",
       "      <td>AUCKLAND</td>\n",
       "      <td>NZAKL</td>\n",
       "      <td>01005136</td>\n",
       "      <td>MONDIALE FREIGHT SERVICES LIMITED-AUCKLAND</td>\n",
       "      <td>2005-07-15</td>\n",
       "      <td>40</td>\n",
       "      <td>HC NOR</td>\n",
       "      <td>2170.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ECS01050600163</td>\n",
       "      <td>CTNR010050700370</td>\n",
       "      <td>PASIR GUDANG</td>\n",
       "      <td>MYPGU</td>\n",
       "      <td>01002767</td>\n",
       "      <td>GLOBELINK CONTAINER LINES (JB) S/B</td>\n",
       "      <td>2005-07-15</td>\n",
       "      <td>40</td>\n",
       "      <td>HC</td>\n",
       "      <td>280.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ECS01050600456</td>\n",
       "      <td>CTNR010050700450</td>\n",
       "      <td>KARACHI (KICT)</td>\n",
       "      <td>PKKHI</td>\n",
       "      <td>01002783</td>\n",
       "      <td>GLOBELINK PAKISTAN (PVT) LTD</td>\n",
       "      <td>2005-07-16</td>\n",
       "      <td>40</td>\n",
       "      <td>HC</td>\n",
       "      <td>1625.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           CSL_ID           CNTR_ID          POD_ID ETD_POL_D  PARTY_ID  \\\n",
       "0  ECS01050600224  CTNR010050700354       ROTTERDAM     NLRTM  01008827   \n",
       "1  ECS01050600610  CTNR010050700353      FELIXSTOWE     GBFXT  01002303   \n",
       "2  ECS01050600041  CTNR010050700351        AUCKLAND     NZAKL  01005136   \n",
       "3  ECS01050600163  CTNR010050700370    PASIR GUDANG     MYPGU  01002767   \n",
       "4  ECS01050600456  CTNR010050700450  KARACHI (KICT)     PKKHI  01002783   \n",
       "\n",
       "                                   PARTY_NAME        POD CNTR_SIZE CNTR_TYPE  \\\n",
       "0             YANG MING (SINGAPORE) PTE. LTD. 2005-07-15        40    HC NOR   \n",
       "1                    GLOBELINK FALLOW LIMITED 2005-07-15        40        GP   \n",
       "2  MONDIALE FREIGHT SERVICES LIMITED-AUCKLAND 2005-07-15        40    HC NOR   \n",
       "3          GLOBELINK CONTAINER LINES (JB) S/B 2005-07-15        40        HC   \n",
       "4                GLOBELINK PAKISTAN (PVT) LTD 2005-07-16        40        HC   \n",
       "\n",
       "     RATE  \n",
       "0  1620.0  \n",
       "1  1800.0  \n",
       "2  2170.0  \n",
       "3   280.0  \n",
       "4  1625.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CSL_ID</th>\n",
       "      <th>CNTR_ID</th>\n",
       "      <th>POD_ID</th>\n",
       "      <th>ETD_POL_D</th>\n",
       "      <th>PARTY_ID</th>\n",
       "      <th>PARTY_NAME</th>\n",
       "      <th>POD</th>\n",
       "      <th>CNTR_SIZE</th>\n",
       "      <th>CNTR_TYPE</th>\n",
       "      <th>RATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ECS01050600224</td>\n",
       "      <td>CTNR010050700354</td>\n",
       "      <td>ROTTERDAM</td>\n",
       "      <td>NLRTM</td>\n",
       "      <td>01008827</td>\n",
       "      <td>YANG MING (SINGAPORE) PTE. LTD.</td>\n",
       "      <td>2005-07-15</td>\n",
       "      <td>40</td>\n",
       "      <td>HC NOR</td>\n",
       "      <td>1620.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ECS01050600610</td>\n",
       "      <td>CTNR010050700353</td>\n",
       "      <td>FELIXSTOWE</td>\n",
       "      <td>GBFXT</td>\n",
       "      <td>01002303</td>\n",
       "      <td>GLOBELINK FALLOW LIMITED</td>\n",
       "      <td>2005-07-15</td>\n",
       "      <td>40</td>\n",
       "      <td>GP</td>\n",
       "      <td>1800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ECS01050600041</td>\n",
       "      <td>CTNR010050700351</td>\n",
       "      <td>AUCKLAND</td>\n",
       "      <td>NZAKL</td>\n",
       "      <td>01005136</td>\n",
       "      <td>MONDIALE FREIGHT SERVICES LIMITED-AUCKLAND</td>\n",
       "      <td>2005-07-15</td>\n",
       "      <td>40</td>\n",
       "      <td>HC NOR</td>\n",
       "      <td>2170.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ECS01050600163</td>\n",
       "      <td>CTNR010050700370</td>\n",
       "      <td>PASIR GUDANG</td>\n",
       "      <td>MYPGU</td>\n",
       "      <td>01002767</td>\n",
       "      <td>GLOBELINK CONTAINER LINES (JB) S/B</td>\n",
       "      <td>2005-07-15</td>\n",
       "      <td>40</td>\n",
       "      <td>HC</td>\n",
       "      <td>280.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ECS01050600456</td>\n",
       "      <td>CTNR010050700450</td>\n",
       "      <td>KARACHI (KICT)</td>\n",
       "      <td>PKKHI</td>\n",
       "      <td>01002783</td>\n",
       "      <td>GLOBELINK PAKISTAN (PVT) LTD</td>\n",
       "      <td>2005-07-16</td>\n",
       "      <td>40</td>\n",
       "      <td>HC</td>\n",
       "      <td>1625.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           CSL_ID           CNTR_ID          POD_ID ETD_POL_D  PARTY_ID  \\\n",
       "0  ECS01050600224  CTNR010050700354       ROTTERDAM     NLRTM  01008827   \n",
       "1  ECS01050600610  CTNR010050700353      FELIXSTOWE     GBFXT  01002303   \n",
       "2  ECS01050600041  CTNR010050700351        AUCKLAND     NZAKL  01005136   \n",
       "3  ECS01050600163  CTNR010050700370    PASIR GUDANG     MYPGU  01002767   \n",
       "4  ECS01050600456  CTNR010050700450  KARACHI (KICT)     PKKHI  01002783   \n",
       "\n",
       "                                   PARTY_NAME        POD CNTR_SIZE CNTR_TYPE  \\\n",
       "0             YANG MING (SINGAPORE) PTE. LTD. 2005-07-15        40    HC NOR   \n",
       "1                    GLOBELINK FALLOW LIMITED 2005-07-15        40        GP   \n",
       "2  MONDIALE FREIGHT SERVICES LIMITED-AUCKLAND 2005-07-15        40    HC NOR   \n",
       "3          GLOBELINK CONTAINER LINES (JB) S/B 2005-07-15        40        HC   \n",
       "4                GLOBELINK PAKISTAN (PVT) LTD 2005-07-16        40        HC   \n",
       "\n",
       "     RATE  \n",
       "0  1620.0  \n",
       "1  1800.0  \n",
       "2  2170.0  \n",
       "3   280.0  \n",
       "4  1625.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import dataset and clean, ready as a dataframe for creating keys\n",
    "def createDF(datasets):\n",
    "    df = pd.read_csv(datasets, converters={\n",
    "                     'PARTY_ID': str, 'COM_ID': str, 'CNTR_SIZE': str})\n",
    "\n",
    "    # Formating to type and remove NaN values\n",
    "    df['POD'] = pd.to_datetime(df['POD'])\n",
    "    df['ENCODED_TYPE'] = df['ENCODED_TYPE'].fillna(-1).astype(int)\n",
    "    df = df.dropna(subset=['ENCODED_TYPE'])\n",
    "    df['RATE'] = df['RATE'].fillna(-1).astype(float)\n",
    "    df = df.dropna(subset=['RATE'])\n",
    "    df['ENCODED_TYPE'] = df['ENCODED_TYPE'].astype(int)\n",
    "    df_clean = df.dropna().reset_index(drop=True)\n",
    "\n",
    "    # Selecting and rearranging columns\n",
    "    sel_col = ['CSL_ID', 'CNTR_ID', 'POD_ID', 'ETD_POL_D', 'PARTY_ID',\n",
    "               'PARTY_NAME', 'POD', 'CNTR_SIZE', 'CNTR_TYPE', 'RATE']\n",
    "    df_fc = df_clean[sel_col]\n",
    "\n",
    "    # Removing years we do not want to process in our models\n",
    "    df_filtered = df_fc[df_fc['POD'].dt.year != 2002]\n",
    "\n",
    "    # Sorting the dates\n",
    "    df_filtered = df_filtered.sort_values(by='POD').reset_index(drop=True)\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "# Create Dataframes for old and new\n",
    "old_data = '.\\Datasets\\CR_COST_FC.csv'\n",
    "df1 = createDF(old_data)\n",
    "df1.head()\n",
    "\n",
    "new_data = '.\\Datasets\\CR_COST_FC_new.csv'\n",
    "df2 = createDF(new_data)\n",
    "df2.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Creating Dictionary Keys</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataframe(df):\n",
    "    filtered_dataframes = {}\n",
    "\n",
    "    for (port, size, ctype, party_id), group in df.groupby(['POD_ID', 'CNTR_SIZE', 'CNTR_TYPE', 'PARTY_ID']):\n",
    "        group = group.reset_index(drop=True).sort_values(by='POD')\n",
    "        df_id = f\"Port_{port}_Size_{size}_Type_{ctype}_PartyID_{party_id}\"\n",
    "        filtered_dataframes[df_id] = group\n",
    "\n",
    "    return filtered_dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating keys from data\n",
    "print(\"Old Data keys:\")\n",
    "filtered_dataframe1 = filter_dataframe(df1)\n",
    "df_ids1 = list(filtered_dataframe1.keys())\n",
    "print(list(df_ids1))\n",
    "print(len(list(df_ids1)))\n",
    "\n",
    "print(\"\\nNew Data keys:\")\n",
    "filtered_dataframe2 = filter_dataframe(df2)\n",
    "df_ids2 = list(filtered_dataframe2.keys())\n",
    "print(list(df_ids2))\n",
    "print(len(list(df_ids2)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Getting ports keys</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKeyPorts(keybunch):\n",
    "    keybunch_pouch = []\n",
    "\n",
    "    # Get a dictionary with key and number of rows for each dataframe in keybunch,\n",
    "    # but only if the number of rows is greater than 1000\n",
    "    key_row_counts = {key: len(keybunch[key])\n",
    "                      for key in keybunch if len(keybunch[key]) > 1000}\n",
    "\n",
    "    # Sort the keys in descending order of the number of rows\n",
    "    sorted_keys = sorted(key_row_counts, key=key_row_counts.get, reverse=True)\n",
    "\n",
    "    for key in sorted_keys:\n",
    "        row_count = key_row_counts[key]\n",
    "        print(f\"Number of rows in {key}: {row_count}\")\n",
    "        keybunch_pouch.append(key)\n",
    "\n",
    "    # Return array of keys\n",
    "    return keybunch_pouch\n",
    "\n",
    "\n",
    "print('Old Dataset Keybunch:')\n",
    "old_df = getKeyPorts(filtered_dataframe1)\n",
    "print('\\n')\n",
    "\n",
    "print('New Dataset Keybunch:')\n",
    "new_df = getKeyPorts(filtered_dataframe2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global selection\n",
    "sel_country = old_df[0]\n",
    "\n",
    "# Accessing the highest count in the each keypouch, new and old.\n",
    "sel_df = filtered_dataframe1[sel_country]\n",
    "sel_df.head(5)\n",
    "sel_df.tail(5)\n",
    "sel_df.info()\n",
    "print(\"\\n\")\n",
    "\n",
    "latest_sel_df = filtered_dataframe2[sel_country]\n",
    "latest_sel_df.head(5)\n",
    "latest_sel_df.tail(5)\n",
    "latest_sel_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features\n",
    "sel_feat = ['POD', 'RATE']\n",
    "# make a copy to avoid SettingWithCopyWarning\n",
    "sel_feat_df = sel_df[sel_feat].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(sel_feat_df['POD'], sel_feat_df['RATE'],\n",
    "         color='blue', label=\"Actual Data\")\n",
    "plt.plot(latest_sel_df['POD'], latest_sel_df['RATE'],\n",
    "         color='red', label=\"New Actual Data\")\n",
    "\n",
    "plt.xlabel('Date(Year Month Week)')\n",
    "plt.ylabel('Cost Rate(USD)')\n",
    "plt.title(sel_country)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Remove Outliers</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "plt.hist(sel_feat_df['RATE'], bins=30, edgecolor='black')\n",
    "plt.title(\"Histogram of RATE\")\n",
    "plt.xlabel(\"RATE\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Q-Q plot\n",
    "stats.probplot(sel_feat_df['RATE'], plot=plt)\n",
    "plt.title(\"Q-Q plot of RATE\")\n",
    "plt.show()\n",
    "\n",
    "# Shapiro-Wilk test\n",
    "print(\"Shapiro-Wilk test:\")\n",
    "W, p = stats.shapiro(sel_feat_df['RATE'])\n",
    "print(f\"W: {W}, p-value: {p}\")\n",
    "\n",
    "# Anderson-Darling test\n",
    "print(\"Anderson-Darling test:\")\n",
    "result = stats.anderson(sel_feat_df['RATE'])\n",
    "print(f\"Statistic: {result.statistic}\")\n",
    "# using a 5% significance level\n",
    "if result.statistic < result.critical_values[2]:\n",
    "    print(\"\\n======================================\")\n",
    "    print(\"\\nData looks normal (fail to reject H0)\")\n",
    "    print(\"Performing Interquartile Range (IQR) to remove outliers...\")\n",
    "    print(\"======================================\\n\")\n",
    "\n",
    "    # IQR\n",
    "    Q1 = sel_feat_df['RATE'].quantile(0.25)\n",
    "    Q3 = sel_feat_df['RATE'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Define bounds for outliers\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Remove outliers\n",
    "    sel_feat_df = sel_feat_df[(sel_feat_df['RATE'] >= lower_bound) & (\n",
    "        sel_feat_df['RATE'] <= upper_bound)]\n",
    "\n",
    "else:\n",
    "    print(\"\\n======================================\")\n",
    "    print(\"\\nData does not look normal (reject H0)\")\n",
    "    print(\"Performing z-score of 3 to remove outliers... \\n\")\n",
    "    print(\"======================================\\n\")\n",
    "\n",
    "    # z-score\n",
    "    z_scores = stats.zscore(sel_feat_df['RATE'])\n",
    "    threshold = 3  # Z-score threshold for outlier detection\n",
    "\n",
    "    # Remove outliers\n",
    "    robust_df = sel_feat_df[(np.abs(z_scores) < threshold)]\n",
    "\n",
    "# Reset the index\n",
    "robust_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the modified DataFrame\n",
    "robust_df.head()\n",
    "robust_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(sel_feat_df['POD'], sel_feat_df['RATE'],\n",
    "         color='blue', label=\"Actual Data\")\n",
    "plt.plot(robust_df['POD'], robust_df['RATE'],\n",
    "         color='red', label=\"Outlier Removed Data\")\n",
    "\n",
    "plt.xlabel('Date(Days)')\n",
    "plt.ylabel('Cost Rate(USD)')\n",
    "plt.title(f'Original Dataset plot: {sel_country}')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Interpolate missing values in between dates</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate missing values\n",
    "\n",
    "# Remove duplicated dates and cost rows\n",
    "robust_df = robust_df.drop_duplicates(\n",
    "    subset=['POD', 'RATE']).reset_index(drop=True)\n",
    "\n",
    "# Create a new dataframe with\n",
    "# a date range from min to max date dataframe\n",
    "new_df = pd.DataFrame()\n",
    "new_df['POD'] = pd.date_range(\n",
    "    start=robust_df['POD'].min(), end=robust_df['POD'].max())\n",
    "\n",
    "# Merge the original dataframe with the new one.\n",
    "# Missing dates in the original dataframe will be filled with NaN\n",
    "df_interpolated = pd.merge(new_df, robust_df, on='POD', how='left')\n",
    "\n",
    "# Perform interpolation polynomial\n",
    "df_interpolated['RATE'] = df_interpolated['RATE'].interpolate(\n",
    "    method='polynomial', order=1)\n",
    "\n",
    "df_interpolated['RATE'] = df_interpolated['RATE'].round(3)\n",
    "\n",
    "df_interpolated.head(5)\n",
    "df_interpolated.tail(5)\n",
    "df_interpolated.info()\n",
    "\n",
    "# Group Weekly\n",
    "# Create YearMonthWeek directly from the 'POD'\n",
    "df_interpolated['YearMonthWeek'] = df_interpolated['POD'] - \\\n",
    "    pd.to_timedelta(df_interpolated['POD'].dt.dayofweek, unit='D')\n",
    "\n",
    "# Create a new dataframe with every week in the range\n",
    "all_weeks = pd.date_range(start=df_interpolated['POD'].min(\n",
    "), end=df_interpolated['POD'].max(), freq='W')\n",
    "all_weeks_df = pd.DataFrame(all_weeks, columns=['POD'])\n",
    "\n",
    "# Create YearMonthWeek in all_weeks_df\n",
    "all_weeks_df['YearMonthWeek'] = all_weeks_df['POD'] - \\\n",
    "    pd.to_timedelta(all_weeks_df['POD'].dt.dayofweek, unit='D')\n",
    "\n",
    "# Merge this with the dataframe\n",
    "merged_df = pd.merge(all_weeks_df, df_interpolated,\n",
    "                     on=['YearMonthWeek'], how='left')\n",
    "\n",
    "# group by YearMonthWeek and compute rate\n",
    "grouped = merged_df.groupby(['YearMonthWeek'])\n",
    "\n",
    "agg_df = pd.DataFrame(columns=['YearMonthWeek', 'Rate'])\n",
    "\n",
    "for group_name, group_df in grouped:\n",
    "    year_month_week = group_name\n",
    "\n",
    "    # Skip if no data for this week\n",
    "    if group_df['RATE'].isnull().all():\n",
    "        continue\n",
    "\n",
    "    # Calculate sum and skewness of RATE values\n",
    "    rate_sum = group_df['RATE'].sum()\n",
    "    rate_skew = group_df['RATE'].skew()\n",
    "\n",
    "    # Handle summed values with skewness\n",
    "    if abs(rate_skew) > 0.5:\n",
    "        rate_metric = group_df['RATE'].median()\n",
    "    else:\n",
    "        rate_metric = group_df['RATE'].mean()\n",
    "\n",
    "    new_row = {\n",
    "        'YearMonthWeek': year_month_week,\n",
    "        'Rate': rate_metric\n",
    "    }\n",
    "\n",
    "    # Append row to aggregated dataframe\n",
    "    agg_df = agg_df.append(new_row, ignore_index=True)\n",
    "\n",
    "agg_df = agg_df.sort_values(by='YearMonthWeek').reset_index(drop=True)\n",
    "agg_df['Rate'] = agg_df['Rate'].round(2)\n",
    "\n",
    "agg_df.head(15)\n",
    "agg_df.tail(15)\n",
    "agg_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group Weekly\n",
    "\n",
    "# Create YearMonthWeek directly from the 'POD'\n",
    "df_interpolated['YearMonthWeek'] = df_interpolated['POD'] - \\\n",
    "    pd.to_timedelta(df_interpolated['POD'].dt.dayofweek, unit='D')\n",
    "\n",
    "# Create a new dataframe with every week in the range\n",
    "all_weeks = pd.date_range(start=df_interpolated['POD'].min(\n",
    "), end=df_interpolated['POD'].max(), freq='W')\n",
    "all_weeks_df = pd.DataFrame(all_weeks, columns=['POD'])\n",
    "\n",
    "# Create YearMonthWeek in all_weeks_df\n",
    "all_weeks_df['YearMonthWeek'] = all_weeks_df['POD'] - \\\n",
    "    pd.to_timedelta(all_weeks_df['POD'].dt.dayofweek, unit='D')\n",
    "\n",
    "# Merge this with the dataframe\n",
    "merged_df = pd.merge(all_weeks_df, df_interpolated,\n",
    "                     on=['YearMonthWeek'], how='left')\n",
    "\n",
    "# group by YearMonthWeek and compute rate\n",
    "grouped = merged_df.groupby(['YearMonthWeek'])\n",
    "\n",
    "agg_df = pd.DataFrame(columns=['YearMonthWeek', 'Rate'])\n",
    "\n",
    "for group_name, group_df in grouped:\n",
    "    year_month_week = group_name\n",
    "\n",
    "    # Skip if no data for this week\n",
    "    if group_df['RATE'].isnull().all():\n",
    "        continue\n",
    "\n",
    "    # Calculate sum and skewness of RATE values\n",
    "    rate_sum = group_df['RATE'].sum()\n",
    "    rate_skew = group_df['RATE'].skew()\n",
    "\n",
    "    # Handle summed values with skewness\n",
    "    if abs(rate_skew) > 0.5:\n",
    "        rate_metric = group_df['RATE'].median()\n",
    "    else:\n",
    "        rate_metric = group_df['RATE'].mean()\n",
    "\n",
    "    new_row = {\n",
    "        'YearMonthWeek': year_month_week,\n",
    "        'Rate': rate_metric\n",
    "    }\n",
    "\n",
    "    # Append row to aggregated dataframe\n",
    "    agg_df = agg_df.append(new_row, ignore_index=True)\n",
    "\n",
    "agg_df = agg_df.sort_values(by='YearMonthWeek').reset_index(drop=True)\n",
    "agg_df['Rate'] = agg_df['Rate'].round(2)\n",
    "\n",
    "agg_df.head(15)\n",
    "agg_df.tail(15)\n",
    "agg_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(sel_df['POD'], sel_df['RATE'], color='blue', label=\"Actual Data\")\n",
    "plt.plot(agg_df['YearMonthWeek'], agg_df['Rate'],\n",
    "         color='red', label=\"Aggregated Data(weeks)\")\n",
    "\n",
    "plt.xlabel('Date(Year Month Week)')\n",
    "plt.ylabel('Cost Rate(USD)')\n",
    "plt.title(sel_country)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Latest datapoints from Latest dataframe for comparing after forecasting (Measure accuracy)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_date_in_old = sel_df['POD'].max()\n",
    "\n",
    "# Create a new dataframe that only includes rows from the latest dataframe\n",
    "# where the date is greater than the maximum date in the old dataframe\n",
    "new_dates_df = latest_sel_df[latest_sel_df['POD']\n",
    "                             > max_date_in_old].reset_index(drop=True)\n",
    "\n",
    "# Print the new dataframe\n",
    "new_dates_df.head(3)\n",
    "new_dates_df.tail(3)\n",
    "new_dates_df.info()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Prepare Features for Feature Engineering</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling mean for 3 weeks and shift of 1 to avoid lookahead bias\n",
    "agg_df['Rate_rolled_3w'] = agg_df['Rate'].rolling(\n",
    "    window=2, min_periods=0).mean().shift(1)\n",
    "\n",
    "# Creating temporal features\n",
    "agg_df['Covid'] = 0\n",
    "agg_df.loc[(agg_df['YearMonthWeek'] >= '2020-01') &\n",
    "           (agg_df['YearMonthWeek'] <= '2023-01'), 'Covid'] = 1\n",
    "\n",
    "# Creating lag features\n",
    "for i in range(1, 4):\n",
    "    agg_df[f'Rate_lag_{i}'] = agg_df['Rate'].shift(i)\n",
    "\n",
    "# Creating rolling mean and std features\n",
    "agg_df['Rate_mean_rolled_3w'] = agg_df['Rate'].rolling(\n",
    "    window=3).mean().shift(1)\n",
    "agg_df['Rate_std_rolled_3w'] = agg_df['Rate'].rolling(window=3).std().shift(1)\n",
    "\n",
    "exogenous_features = ['Rate_rolled_3w', 'Covid',\n",
    "                      'Rate_lag_1', 'Rate_lag_2', 'Rate_lag_3',\n",
    "                      'Rate_mean_rolled_3w', 'Rate_std_rolled_3w']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(sel_df['POD'], sel_df['RATE'], color='green', label=\"Actual Data\")\n",
    "plt.plot(agg_df['YearMonthWeek'], agg_df['Rate'],\n",
    "         color='blue', label=\"Aggregated Data(weeks)\")\n",
    "\n",
    "# Fill the area between the graph and x-axis when Covid is 1\n",
    "plt.fill_between(agg_df['YearMonthWeek'], agg_df['Rate'],\n",
    "                 where=agg_df['Covid'] == 1, color='red', alpha=0.3, label=\"Covid Period\")\n",
    "\n",
    "\n",
    "plt.xlabel('Date(Year Month Week)')\n",
    "plt.ylabel('Cost Rate(USD)')\n",
    "plt.title(sel_country)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>ARIMA<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sarimax without train test split and with Feature engineering\n",
    "\n",
    "# Mean Square Error Function:\n",
    "def calculate_RMSE(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# Function to execute Auto ARIMA model and return RMSE on the given data\n",
    "\n",
    "\n",
    "def ARIMA_Execute(endog, exog):\n",
    "    # Fit an auto_arima model\n",
    "    arima_model = pm.auto_arima(endog, exogenous=exog, seasonal=True, trace=True,\n",
    "                                error_action='ignore',\n",
    "                                suppress_warnings=True,\n",
    "                                stepwise=True, alpha=0.07, m=7)\n",
    "    # Print the summary of the model\n",
    "    print(arima_model.summary())\n",
    "\n",
    "    # Forecast\n",
    "    data_forecast = arima_model.predict_in_sample(exogenous=exog)\n",
    "\n",
    "    # Calculate the RMSE\n",
    "    RMSE_ARIMA_data = calculate_RMSE(endog, data_forecast)\n",
    "    print(\"RMSE: %.3f\" % RMSE_ARIMA_data)\n",
    "\n",
    "    return arima_model, RMSE_ARIMA_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For: Sarimax without train test split but with Feature Engineering\n",
    "\n",
    "# Get the 'Rate' column values as endogenous variable\n",
    "endog = agg_df['Rate'].values\n",
    "\n",
    "# Get the exogenous features\n",
    "exog = agg_df[exogenous_features].values\n",
    "\n",
    "# Run ARIMA model\n",
    "model, ARIMA_rmse = ARIMA_Execute(endog, exog)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Forecast the results</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add check for 'RATE_actual' values to avoid division by zero\n",
    "def compute_accuracy(row):\n",
    "    if row['RATE_actual'] == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        error = abs(row['RATE_actual'] - row['RATE_forecasted'])\n",
    "        error_proportion = error / row['RATE_actual']\n",
    "        return (1 - error_proportion) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast next 12 weeks using ARIMA model\n",
    "weeks = 12\n",
    "forecasted_values = model.predict(n_periods=weeks)\n",
    "\n",
    "# Ensure that 'YearMonthWeek' is a datetime object\n",
    "agg_df['YearMonthWeek'] = pd.to_datetime(agg_df['YearMonthWeek'])\n",
    "last_date = agg_df['YearMonthWeek'].iloc[-1]\n",
    "forecasted_dates = pd.date_range(\n",
    "    start=last_date, periods=weeks+1, freq='W')[1:]\n",
    "\n",
    "df_forecasted = pd.DataFrame({\n",
    "    'POD': forecasted_dates,\n",
    "    'RATE': forecasted_values.ravel()\n",
    "})\n",
    "\n",
    "df_forecasted[\"RATE\"] = df_forecasted[\"RATE\"].round(2)\n",
    "\n",
    "df_forecasted.head(5)\n",
    "df_forecasted.tail(5)\n",
    "df_forecasted.info()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Comparing with actual updated against forecasted</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.DataFrame(\n",
    "    columns=['WeekStart', 'WeekEnd', 'POD_actual', 'RATE_forecasted', 'RATE_actual'])\n",
    "df_forecasted['WeekEnd'] = df_forecasted['POD'] + pd.to_timedelta(7, unit='d')\n",
    "\n",
    "for _, row in df_forecasted.iterrows():\n",
    "    mask = (new_dates_df['POD'] >= row['POD']) & (\n",
    "        new_dates_df['POD'] < row['WeekEnd'])\n",
    "    actual_dates_within_week = new_dates_df[mask]\n",
    "\n",
    "    for _, actual_row in actual_dates_within_week.iterrows():\n",
    "        comparison_df = comparison_df.append({\n",
    "            'WeekStart': row['POD'],\n",
    "            'WeekEnd': row['WeekEnd'],\n",
    "            'POD_actual': actual_row['POD'],\n",
    "            'RATE_forecasted': row['RATE'],\n",
    "            'RATE_actual': actual_row['RATE']\n",
    "        }, ignore_index=True)\n",
    "\n",
    "# Remove duplicates\n",
    "comparison_df = comparison_df.drop_duplicates(\n",
    "    subset=['POD_actual', 'RATE_forecasted', 'RATE_actual']).reset_index(drop=True)\n",
    "\n",
    "# Compute accuracy\n",
    "comparison_df['accuracy'] = comparison_df.apply(compute_accuracy, axis=1)\n",
    "comparison_df = comparison_df.dropna(subset=['accuracy'])\n",
    "\n",
    "total_mean_accuracy = comparison_df['accuracy'].mean()\n",
    "comparison_df\n",
    "print(f'The mean accuracy is {total_mean_accuracy:.2f}%\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Visualise all, Conclusion</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(sel_df['POD'], sel_df['RATE'], color='blue', label=\"Actual Data\")\n",
    "plt.plot(new_dates_df['POD'], new_dates_df['RATE'],\n",
    "         color='blue', label=\"Actual Data (Updated)\")\n",
    "\n",
    "plt.plot(df_interpolated['POD'], df_interpolated['RATE'],\n",
    "         color='green', label=\"Aggregated Data\")\n",
    "plt.plot(df_forecasted['POD'], df_forecasted['RATE'],\n",
    "         color='red', label=\"Forecasted Data\")\n",
    "\n",
    "plt.xlabel('Date(Year Month)')\n",
    "plt.ylabel('Cost Rate(USD)')\n",
    "plt.title(sel_country)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
