{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "# LSTM\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from sklearn.svm import OneClassSVM\n",
    "from tensorflow.keras.layers import Dropout, LSTM, Dense, RepeatVector, TimeDistributed\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from scipy.stats import skew\n",
    "\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Importing Datasets</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset and clean, ready as a dataframe for creating keys\n",
    "def createDF(datasets):\n",
    "    df = pd.read_csv(datasets, converters={'PARTY_ID': str, 'COM_ID': str, 'CNTR_SIZE': str})\n",
    "\n",
    "    # Formating to type and remove NaN values\n",
    "    df['POD'] = pd.to_datetime(df['POD'])\n",
    "    df['ENCODED_TYPE'] = df['ENCODED_TYPE'].fillna(-1).astype(int)\n",
    "    df = df.dropna(subset=['ENCODED_TYPE'])\n",
    "    df['RATE'] = df['RATE'].fillna(-1).astype(float)\n",
    "    df = df.dropna(subset=['RATE'])\n",
    "    df['ENCODED_TYPE'] = df['ENCODED_TYPE'].astype(int)\n",
    "    df_clean= df.dropna().reset_index(drop=True)\n",
    "\n",
    "    # Selecting and rearranging columns\n",
    "    sel_col = ['CSL_ID', 'CNTR_ID','POD_ID','ETD_POL_D','PARTY_ID',\n",
    "            'PARTY_NAME','POD','CNTR_SIZE','CNTR_TYPE','RATE']\n",
    "    df_fc = df_clean[sel_col]\n",
    "\n",
    "    # Removing years we do not want to process in our models\n",
    "    df_filtered = df_fc[df_fc['POD'].dt.year != 2002]\n",
    "\n",
    "    # Sorting the dates\n",
    "    df_filtered = df_filtered.sort_values(by='POD').reset_index(drop=True)\n",
    "    \n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataframes for old and new\n",
    "old_data = '.\\Datasets\\CR_COST_FC.csv'\n",
    "df1 = createDF(old_data)\n",
    "df1.head()\n",
    "\n",
    "new_data = '.\\Datasets\\CR_COST_FC_new.csv'\n",
    "df2 = createDF(new_data)\n",
    "df2.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Creating Dictionary Keys</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataframe(df):\n",
    "    filtered_dataframes = {}\n",
    "\n",
    "    for (port, size, ctype, party_id), group in df.groupby(['POD_ID', 'CNTR_SIZE', 'CNTR_TYPE', 'PARTY_ID']):\n",
    "        group = group.reset_index(drop=True).sort_values(by='POD')\n",
    "        df_id = f\"Port_{port}_Size_{size}_Type_{ctype}_PartyID_{party_id}\"\n",
    "        filtered_dataframes[df_id] = group\n",
    "\n",
    "    return filtered_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating keys from data\n",
    "print(\"Old Data keys:\")\n",
    "filtered_dataframe1 = filter_dataframe(df1)\n",
    "df_ids1 = list(filtered_dataframe1.keys())\n",
    "print(list(df_ids1))\n",
    "print(len(list(df_ids1)))\n",
    "\n",
    "print(\"\\nNew Data keys:\")\n",
    "filtered_dataframe2 = filter_dataframe(df2)\n",
    "df_ids2 = list(filtered_dataframe2.keys())\n",
    "print(list(df_ids2))\n",
    "print(len(list(df_ids2)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Getting Top 5 ports keys</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTop5Ports(keybunch):\n",
    "    keybunch_pouch = []\n",
    "    \n",
    "    # Get a dictionary with key and number of rows for each dataframe in filtered_dataframes\n",
    "    key_row_counts = {key: len(keybunch[key]) for key in keybunch}\n",
    "\n",
    "    # Sort the key_row_counts dictionary by value (number of rows) in descending order\n",
    "    sorted_key_row_counts = sorted(key_row_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "    # Get the top 5 keys with the most rows\n",
    "    top_5_keys_tuple = sorted_key_row_counts[:5]\n",
    "\n",
    "    # Create a dictionary with the top 5 keys and their corresponding dataframes (with up to 5 rows per dataframe)\n",
    "    keybunch_subset = {}\n",
    "\n",
    "    for key, row_count in top_5_keys_tuple:\n",
    "        keybunch_subset[key] = keybunch[key][:5]\n",
    "        print(f\"Number of rows in {key}: {row_count}\")\n",
    "        keybunch_pouch.append(key)\n",
    "    \n",
    "    # Return array of keys\n",
    "    return keybunch_pouch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Old Dataset Keybunch:')\n",
    "old_df = getTop5Ports(filtered_dataframe1)\n",
    "print('\\n')\n",
    "\n",
    "print('New Dataset Keybunch:')\n",
    "new_df = getTop5Ports(filtered_dataframe2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global selection\n",
    "sel_country = old_df[2]\n",
    "\n",
    "# Accessing the highest count in the each keypouch, new and old.\n",
    "sel_df = filtered_dataframe1[sel_country]\n",
    "sel_df.head(5)\n",
    "sel_df.tail(5)\n",
    "sel_df.info()\n",
    "print(\"\\n\")\n",
    "\n",
    "latest_sel_df = filtered_dataframe2[sel_country]\n",
    "latest_sel_df.head(5)\n",
    "latest_sel_df.tail(5)\n",
    "latest_sel_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features\n",
    "sel_feat = ['POD','RATE']\n",
    "robust_df = sel_df[sel_feat].copy()  # make a copy to avoid SettingWithCopyWarning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Interpolate missing values in between dates</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated dates and cost rows\n",
    "robust_df = robust_df.drop_duplicates(subset=['POD', 'RATE']).reset_index(drop=True)\n",
    "\n",
    "# Create a new dataframe with a date range from min to max date in your dataframe\n",
    "new_df = pd.DataFrame()\n",
    "new_df['POD'] = pd.date_range(start=robust_df['POD'].min(), end=robust_df['POD'].max())\n",
    "\n",
    "# Merge the original dataframe with the new one. Missing dates in the original dataframe will be filled with NaN\n",
    "df_interpolated = pd.merge(new_df, robust_df, on='POD', how='left')  \n",
    "\n",
    "# Perform spline interpolation\n",
    "df_interpolated['RATE'] = df_interpolated['RATE'].interpolate(method='polynomial', order=1)\n",
    "\n",
    "df_interpolated['RATE'] = df_interpolated['RATE'].round(3)\n",
    "\n",
    "df_interpolated.head(5)\n",
    "df_interpolated.tail(5)\n",
    "df_interpolated.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Grouping it to week</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Create YearMonthWeek directly from the 'POD'\n",
    "df_interpolated['YearMonthWeek'] = df_interpolated['POD'] - pd.to_timedelta(df_interpolated['POD'].dt.dayofweek, unit='D')\n",
    "\n",
    "# Create a new dataframe with every week in the range\n",
    "all_weeks = pd.date_range(start=df_interpolated['POD'].min(), end=df_interpolated['POD'].max(), freq='W')\n",
    "all_weeks_df = pd.DataFrame(all_weeks, columns=['POD'])\n",
    "\n",
    "# Create YearMonthWeek in all_weeks_df\n",
    "all_weeks_df['YearMonthWeek'] = all_weeks_df['POD'] - pd.to_timedelta(all_weeks_df['POD'].dt.dayofweek, unit='D')\n",
    "\n",
    "# Merge this with your original dataframe\n",
    "merged_df = pd.merge(all_weeks_df, df_interpolated, on=['YearMonthWeek'], how='left')\n",
    "\n",
    "# Now you can group by YearMonthWeek and compute your rate\n",
    "grouped = merged_df.groupby(['YearMonthWeek'])\n",
    "\n",
    "agg_df = pd.DataFrame(columns=['YearMonthWeek', 'Rate'])\n",
    "\n",
    "for group_name, group_df in grouped:\n",
    "    year_month_week = group_name\n",
    "\n",
    "    # Skip if no data for this week\n",
    "    if group_df['RATE'].isnull().all():\n",
    "        continue\n",
    "\n",
    "    # Calculate sum and skewness of RATE values\n",
    "    rate_sum = group_df['RATE'].sum()\n",
    "    rate_skew = group_df['RATE'].skew()\n",
    "\n",
    "    # Calculate trimmed mean of RATE values\n",
    "    rate_metric = stats.trim_mean(group_df['RATE'].dropna().values, 0.1) # trimming 10% from each end\n",
    "\n",
    "    new_row = {\n",
    "        'YearMonthWeek': year_month_week,\n",
    "        'Rate': rate_metric\n",
    "    }\n",
    "\n",
    "    # Append row to aggregated dataframe\n",
    "    agg_df = agg_df.append(new_row, ignore_index=True)\n",
    "\n",
    "agg_df = agg_df.sort_values(by='YearMonthWeek').reset_index(drop=True)\n",
    "agg_df['Rate'] = agg_df['Rate'].round(2)\n",
    "\n",
    "agg_df.head(15)\n",
    "agg_df.tail(15)\n",
    "agg_df.info()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Latest datapoints from Latest dataframe for comparing after forecasting (Measure accuracy)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_date_in_old = sel_df['POD'].max()\n",
    "\n",
    "# Create a new dataframe that only includes rows from the latest dataframe where the date is greater than the maximum date in the old dataframe\n",
    "new_dates_df = latest_sel_df[latest_sel_df['POD'] > max_date_in_old].reset_index(drop=True)\n",
    "\n",
    "# Print the new dataframe\n",
    "new_dates_df.head(3)\n",
    "new_dates_df.tail(3)\n",
    "new_dates_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(sel_df['POD'], sel_df['RATE'], color='blue', label=\"Actual Data\")\n",
    "plt.plot(agg_df['YearMonthWeek'], agg_df['Rate'], color='red', label=\"Aggregated Data(weeks)\")\n",
    "\n",
    "plt.xlabel('Date(Year Month Week)')\n",
    "plt.ylabel('Cost Rate(USD)')\n",
    "plt.title(sel_country)\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Annomaly Detection using LSTM Autoencoder with OCSVM</h2>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Finding best params for LSTM auto-encoder and nu for one class svm</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a result list\n",
    "results = []\n",
    "\n",
    "# Define the hyperparameters grid\n",
    "param_grid = {\n",
    "    'lstm_units': [64, 128],\n",
    "    'time_steps': [10, 20],\n",
    "    'batch_size': [16, 32],\n",
    "    'epochs': [30, 50]\n",
    "}\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=5)\n",
    "\n",
    "# Create a random hyperparameters sampler\n",
    "param_list = list(ParameterSampler(param_grid, n_iter=10, random_state=42))\n",
    "\n",
    "# Start the manual random search\n",
    "for params in param_list:\n",
    "    # Extract parameters from current combination\n",
    "    lstm_units, time_steps, batch_size, epochs = params['lstm_units'], params['time_steps'], params['batch_size'], params['epochs']\n",
    "    \n",
    "    # Create generator with current time_steps\n",
    "    series_gen = TimeseriesGenerator(data_scaled, data_scaled, length=time_steps, batch_size=batch_size)\n",
    "    \n",
    "    # Create and train model with current hyperparameters\n",
    "    model = Sequential([\n",
    "        LSTM(lstm_units, activation='relu', input_shape=(time_steps, 1), return_sequences=False),\n",
    "        RepeatVector(time_steps),\n",
    "        LSTM(lstm_units, activation='relu', return_sequences=True),\n",
    "        TimeDistributed(Dense(1))\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    history = model.fit(series_gen, epochs=epochs, verbose=0, callbacks=[early_stopping])\n",
    "    \n",
    "    # Calculate final loss\n",
    "    final_loss = history.history['loss'][-1]\n",
    "    \n",
    "    # Append current hyperparameters and result to results list\n",
    "    results.append((params, final_loss))\n",
    "\n",
    "# Print results\n",
    "for params, final_loss in results:\n",
    "    print(f'Parameters: {params}, loss={final_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of possible nu values\n",
    "nu_values = [0.01, 0.02, 0.05, 0.1, 0.2, 0.5]\n",
    "\n",
    "best_nu = 0\n",
    "best_score = float('inf')\n",
    "\n",
    "# Loop over the nu_values\n",
    "for nu in nu_values:\n",
    "    # Using One-Class SVM for anomaly detection\n",
    "    ocsvm = OneClassSVM(nu=nu)\n",
    "    ocsvm.fit(errors.reshape(-1, 1))\n",
    "    anomaly_flags = ocsvm.predict(errors.reshape(-1, 1))\n",
    "\n",
    "    # Count the number of anomalies detected\n",
    "    num_anomalies = sum(anomaly_flags == -1)\n",
    "\n",
    "    # Calculate the mean square error of anomalies\n",
    "    mse_anomalies = np.mean(errors[anomaly_flags == -1])\n",
    "\n",
    "    # Combine num_anomalies and mse_anomalies into a score\n",
    "    # This is just one example of how to score. You should adjust this to fit your needs.\n",
    "    score = num_anomalies * mse_anomalies\n",
    "\n",
    "    # If the score for this nu value is better than our current best, update best_nu and best_score\n",
    "    if score < best_score:\n",
    "        best_nu = nu\n",
    "        best_score = score\n",
    "\n",
    "    print(f\"nu: {nu}, score: {score}\")\n",
    "\n",
    "print(f\"Best nu: {best_nu}, best score: {best_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: scaling data to [0, 1] range\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(agg_df['Rate'].values.reshape(-1, 1))\n",
    "\n",
    "# Extracting parameters from params\n",
    "time_steps = params['time_steps']\n",
    "batch_size = params['batch_size']\n",
    "lstm_units = params['lstm_units']\n",
    "epochs = params['epochs']\n",
    "\n",
    "# Creating a series generator for the LSTM autoencoder\n",
    "series_gen = TimeseriesGenerator(data_scaled, data_scaled,\n",
    "                                 length=time_steps, batch_size=batch_size)\n",
    "\n",
    "# LSTM autoencoder\n",
    "model = Sequential([\n",
    "    LSTM(lstm_units, activation='relu', input_shape=(time_steps, 1), return_sequences=False),\n",
    "    RepeatVector(time_steps),\n",
    "    LSTM(lstm_units, activation='relu', return_sequences=True),\n",
    "    TimeDistributed(Dense(1))\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(series_gen, epochs=epochs)\n",
    "\n",
    "# Calculating reconstruction error\n",
    "preds = model.predict(series_gen)\n",
    "\n",
    "# Reshape data_scaled to match preds' shape\n",
    "data_scaled_reshaped = np.array([data_scaled[i-time_steps:i] for i in range(time_steps, len(data_scaled))])\n",
    "\n",
    "# Calculate mean square error\n",
    "errors = np.mean(np.square(preds - data_scaled_reshaped), axis=(1, 2))\n",
    "\n",
    "# Using One-Class SVM for anomalyetection\n",
    "ocsvm = OneClassSVM(nu=best_nu)\n",
    "ocsvm.fit(errors.reshape(-1, 1))\n",
    "anomaly_flags = ocsvm.predict(errors.reshape(-1, 1))\n",
    "\n",
    "# Marking anomalies in the data\n",
    "agg_df['Anomaly'] = 0\n",
    "agg_df.iloc[time_steps:len(preds) + time_steps, agg_df.columns.get_loc('Anomaly')] = anomaly_flags\n",
    "\n",
    "# If you want to exclude anomalies from the data:\n",
    "df_no_anomalies = agg_df[agg_df['Anomaly'] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(agg_df['YearMonthWeek'], agg_df['Rate'], color='blue', label=\"Data\")\n",
    "\n",
    "# Plot Anomalies\n",
    "anomalies = agg_df[agg_df['Anomaly'] == 1]\n",
    "plt.plot(anomalies['YearMonthWeek'], anomalies['Rate'], 'ro', markersize=4, label=\"Anomaly\")\n",
    "\n",
    "plt.xlabel('Date(Year Month Week)')\n",
    "plt.ylabel('Cost Rate(USD)')\n",
    "plt.title(sel_country)\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Handling Anomaly</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate skewness\n",
    "skewness = skew(agg_df['Rate'])\n",
    "\n",
    "# if skewness is high, fill anomalies with median, else fill with mean\n",
    "if abs(skewness) > 0.5:\n",
    "    agg_df.loc[agg_df['Anomaly'] == -1, 'Rate'] = agg_df['Rate'].median()\n",
    "else:\n",
    "    agg_df.loc[agg_df['Anomaly'] == -1, 'Rate'] = agg_df['Rate'].mean()\n",
    "    \n",
    "# Plot to visualise\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(sel_df['POD'], sel_df['RATE'], color='blue', label=\"Actual Data\")\n",
    "plt.plot(agg_df['YearMonthWeek'], agg_df['Rate'], color='Red', label=\"Data after handling anomaly\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>LSTM Regression<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Mean Square Error Function:\n",
    "def calculate_RMSE(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# Update create_dataset to handle multi-feature dataset\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - look_back - 1):\n",
    "        a = dataset[i:(i + look_back), :]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "\n",
    "def create_LSTM_model(trainX, trainY, testX, testY, epochs, lstm_layers):\n",
    "    model = Sequential()\n",
    "    \n",
    "    for i in range(len(lstm_layers)):\n",
    "        if i == 0:\n",
    "            # First layer specifies the input_shape and returns sequences\n",
    "            model.add(LSTM(lstm_layers[i], return_sequences=True, input_shape=(look_back, trainX.shape[2]), \n",
    "                           activation='tanh', recurrent_activation='hard_sigmoid', kernel_regularizer=l2(0.01)))\n",
    "        elif i == len(lstm_layers) - 1:\n",
    "            # Last layer doesn't return sequences\n",
    "            model.add(LSTM(lstm_layers[i], activation='tanh', kernel_regularizer=l2(0.01)))\n",
    "        else:\n",
    "            # Middle layers return sequences\n",
    "            model.add(LSTM(lstm_layers[i], return_sequences=True, activation='tanh', \n",
    "                           recurrent_activation='hard_sigmoid', kernel_regularizer=l2(0.01)))\n",
    "        model.add(Dropout(0.2))\n",
    "        \n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.0005))\n",
    "    model.summary()\n",
    "\n",
    "    # Add early stopping\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "    \n",
    "    # Fit the model and store the history\n",
    "    history = model.fit(trainX, trainY, epochs=epochs, batch_size=128, verbose=2, \n",
    "                        validation_data=(testX, testY), callbacks=[es])\n",
    "    return model, history\n",
    "\n",
    "\n",
    "def plot_train_val_loss(history):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model loss progress during training and validation')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize dataset for LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize two separate scalers\n",
    "scaler_rate = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit and transform 'RATE' and 'during_covid' separately\n",
    "rate_scaled = scaler_rate.fit_transform(agg_df[['Rate']])\n",
    "\n",
    "# Concatenate the scaled data\n",
    "dataset = np.concatenate([rate_scaled], axis=1)\n",
    "\n",
    "# Split into train and test sets\n",
    "train_size = int(len(dataset) * 0.7)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "\n",
    "# Reshape into X=t and Y=t+1, timestep  look_back\n",
    "look_back = 5\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "# Reshape input to be [samples, time steps, features]\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], look_back, trainX.shape[2]))\n",
    "testX = np.reshape(testX, (testX.shape[0], look_back, testX.shape[2]))\n",
    "\n",
    "epochs_list = [50, 100, 150]\n",
    "\n",
    "lstm_layers_list = [\n",
    "    [64, 64, 32, 32, 16, 16, 8, 8, 4, 4, 2, 2],\n",
    "    [32, 32, 16, 16, 8, 8, 4, 4, 2, 2],\n",
    "    [16, 16, 8, 8, 4, 4, 2, 2],\n",
    "    [8, 8, 4, 4, 2, 2],\n",
    "    [4, 4, 2, 2],\n",
    "    [2, 2]\n",
    "]\n",
    "\n",
    "rmse_results = {}\n",
    "\n",
    "for epochs in epochs_list:\n",
    "    print(f'Training for {epochs} epochs...')\n",
    "    \n",
    "    for lstm_layers in lstm_layers_list:\n",
    "        print(f'Training with LSTM layers: {lstm_layers}')\n",
    "        model, history = create_LSTM_model(trainX, trainY, testX, testY, epochs, lstm_layers)\n",
    "        \n",
    "        # Add the loss for this model to the plot\n",
    "        plt.plot(history.history['loss'], label=f'Train Loss - {epochs} epochs, layers: {lstm_layers}')\n",
    "        plt.plot(history.history['val_loss'], label=f'Validation Loss - {epochs} epochs, layers: {lstm_layers}')\n",
    "\n",
    "        # Evalute LSTM Model\n",
    "        trainPredict = model.predict(trainX)\n",
    "        testPredict = model.predict(testX)\n",
    "\n",
    "        # inverse_transform\n",
    "        trainPredict = scaler.inverse_transform(trainPredict)\n",
    "        trainY_orig = scaler.inverse_transform([trainY])\n",
    "        testPredict = scaler.inverse_transform(testPredict)\n",
    "        testY_orig = scaler.inverse_transform([testY])\n",
    "\n",
    "        # Calculate mean squared error\n",
    "        trainScore = calculate_RMSE(trainY_orig[0], trainPredict[:,0])\n",
    "        print(f'Train Score: {trainScore:.2f} RMSE for {epochs} epochs')\n",
    "        testScore = calculate_RMSE(testY_orig[0], testPredict[:,0])\n",
    "        print(f'Test Score: {testScore:.2f} RMSE for {epochs} epochs')\n",
    "\n",
    "        rmse_results[f'{epochs} epochs, {lstm_layers} layers'] = {'Train RMSE': trainScore, 'Test RMSE': testScore}\n",
    "\n",
    "# Configure and show the plot\n",
    "plt.title('Model loss progress during training and validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show();\n",
    "\n",
    "# Convert the dictionary to a DataFrame for easy display\n",
    "rmse_df = pd.DataFrame(rmse_results).T\n",
    "print(rmse_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Forecast the results</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add check for 'RATE_actual' values to avoid division by zero\n",
    "def compute_accuracy(row):\n",
    "    if row['RATE_actual'] == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        error = abs(row['RATE_actual'] - row['RATE_forecasted'])\n",
    "        error_proportion = error / row['RATE_actual']\n",
    "        return (1 - error_proportion) * 100\n",
    "\n",
    "def forecast_next_weeks(model, look_back, scaler, last_values, n_weeks):\n",
    "    forecast = []\n",
    "    for _ in range(n_weeks):\n",
    "        last_values_2d = np.array(last_values[-look_back:]).reshape(-1, 1)\n",
    "        input_values_scaled = scaler.transform(last_values_2d)\n",
    "        input_values_scaled = input_values_scaled.reshape((1, look_back, 1))\n",
    "        input_values_scaled = np.repeat(input_values_scaled, 2, axis=-1)\n",
    "        input_values_scaled[..., 1] = 0\n",
    "\n",
    "        prediction = model.predict(input_values_scaled)\n",
    "        prediction = scaler.inverse_transform(prediction)\n",
    "        forecast.append(prediction[0][0])\n",
    "        last_values.append(prediction[0][0])\n",
    "\n",
    "    return forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks = 12\n",
    "\n",
    "# Ensure that 'YearMonthWeek' is a datetime object\n",
    "agg_df['YearMonthWeek'] = pd.to_datetime(agg_df['YearMonthWeek'])\n",
    "last_date = agg_df['YearMonthWeek'].iloc[-1]\n",
    "\n",
    "last_values = list(agg_df['Rate'].values[-look_back:])\n",
    "forecasted_values = forecast_next_weeks(model, look_back, scaler, last_values, weeks)\n",
    "forecasted_dates = pd.date_range(start=last_date, periods=weeks+1, freq='W')[1:]\n",
    "\n",
    "df_forecasted = pd.DataFrame({\n",
    "    'POD': forecasted_dates,\n",
    "    'RATE': forecasted_values\n",
    "})\n",
    "\n",
    "df_forecasted[\"RATE\"] = df_forecasted[\"RATE\"].round(2)\n",
    "\n",
    "df_forecasted.head(5)\n",
    "df_forecasted.tail(5)\n",
    "df_forecasted.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Comparing with actual updated against forecasted</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.DataFrame(columns=['WeekStart', 'WeekEnd', 'POD_actual', 'RATE_forecasted', 'RATE_actual'])\n",
    "df_forecasted['WeekEnd'] = df_forecasted['POD'] + pd.to_timedelta(7, unit='d')  \n",
    "\n",
    "for _, row in df_forecasted.iterrows():\n",
    "    mask = (new_dates_df['POD'] >= row['POD']) & (new_dates_df['POD'] < row['WeekEnd'])\n",
    "    actual_dates_within_week = new_dates_df[mask]\n",
    "\n",
    "    for _, actual_row in actual_dates_within_week.iterrows():\n",
    "        comparison_df = comparison_df.append({\n",
    "            'WeekStart': row['POD'],\n",
    "            'WeekEnd': row['WeekEnd'],\n",
    "            'POD_actual': actual_row['POD'],\n",
    "            'RATE_forecasted': row['RATE'],\n",
    "            'RATE_actual': actual_row['RATE']\n",
    "        }, ignore_index=True)\n",
    "\n",
    "# Remove duplicates\n",
    "comparison_df = comparison_df.drop_duplicates(subset=['POD_actual', 'RATE_forecasted', 'RATE_actual']).reset_index(drop=True)\n",
    "\n",
    "# Compute accuracy\n",
    "comparison_df['accuracy'] = comparison_df.apply(compute_accuracy, axis=1)\n",
    "comparison_df = comparison_df.dropna(subset=['accuracy'])\n",
    "\n",
    "total_mean_accuracy = comparison_df['accuracy'].mean()\n",
    "comparison_df\n",
    "print(f'The mean accuracy is {total_mean_accuracy:.2f}%\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Visualise all, Conclusion</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(sel_df['POD'], sel_df['RATE'], color='blue', label=\"Actual Data\")\n",
    "plt.plot(new_dates_df['POD'], new_dates_df['RATE'], color='blue', label=\"Actual Data (Updated)\")\n",
    "\n",
    "plt.plot(df_interpolated['POD'], df_interpolated['RATE'], color='green', label=\"Aggregated Data\")\n",
    "plt.plot(df_forecasted['POD'], df_forecasted['RATE'], color='red', label=\"Forecasted Data\")\n",
    "\n",
    "plt.xlabel('Date(Year Month)')\n",
    "plt.ylabel('Cost Rate(USD)')\n",
    "plt.title('Port_BUSAN_Size_40_Type_HC_PartyID_010004286')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
