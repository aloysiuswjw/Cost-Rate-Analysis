{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "pd.options.display.max_rows = 100\n",
    "\n",
    "df = pd.read_csv('CR_COST_FC.csv', converters={'PARTY_ID': str, 'COM_ID': str, 'CNTR_SIZE': str})\n",
    "df['POD'] = pd.to_datetime(df['POD'])\n",
    "df['ENCODED_TYPE'] = df['ENCODED_TYPE'].fillna(-1).astype(int)\n",
    "df = df.dropna(subset=['ENCODED_TYPE'])\n",
    "df['RATE'] = df['RATE'].fillna(-1).astype(int)\n",
    "df = df.dropna(subset=['RATE'])\n",
    "df['ENCODED_TYPE'] = df['ENCODED_TYPE'].astype(int)\n",
    "\n",
    "\n",
    "df.head()\n",
    "df.info()\n",
    "print(f'Dataset size: {df.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Checking if dataset still contains any NAN values after datacleaning using SQL</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df.isna().any().any():\n",
    "    print('Dataframe contains NaN values')\n",
    "else:\n",
    "    print('Dataframe does not contain NaN values')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Removing rows contains NAN values</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.dropna().reset_index(drop=True)\n",
    "df_clean.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Casting All to INT32 or INT64 Type</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['CNTR_SIZE'] = df_clean['CNTR_SIZE'].astype(np.int32)\n",
    "df_clean['RATE'] = df_clean['RATE'].astype(np.int32)\n",
    "df_clean['PARTY_ID_EN'] = df_clean['PARTY_ID_EN'].astype(np.int32)\n",
    "df_clean['POD_ID_EN'] = df_clean['POD_ID_EN'].astype(np.int64)\n",
    "df_clean['ETA_ETD_NO'] = df_clean['ETA_ETD_NO'].astype(np.int32)\n",
    "df_clean.info()\n",
    "df_clean.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Rearrange columns where ID first then label</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_col = ['COM_ID','CSL_ID', 'CNTR_ID','ENCODED_TERM','COST_TERM','POD_ID','ETD_POL_D','PARTY_ID',\n",
    "           'PARTY_ID_EN', 'PARTY_NAME','POD_ID_EN','ETA_ETD_NO','POD',\n",
    "           'CNTR_SIZE','ENCODED_TYPE','CNTR_TYPE','RATE']\n",
    "\n",
    "df_fc = df_clean[sel_col]\n",
    "df_fc.head()\n",
    "df_fc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate missing values\n",
    "df_fc['POD'].dt.year.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting by date so can interpolate the missing dates while using cost median \n",
    "df_fc = df_fc.sort_values(by='POD').reset_index(drop=True)\n",
    "df_fc.head()\n",
    "df_fc['POD'].dt.year.unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Removing year 2002 as data before 2005 was used for data testing. Hence, it is not relevant.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out rows where the year is 2002\n",
    "df_filtered = df_fc[df_fc['POD'].dt.year != 2002]\n",
    "df_filtered.head()\n",
    "df_filtered.info()\n",
    "\n",
    "# Checking if year 2002 is removed\n",
    "df_filtered['POD'].dt.year.unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Filtering by most export count, container size, container type and focus in on most used shipping vendor quotations.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View which is the most popular port using numpy\n",
    "port_id, port_count = np.unique(df_filtered[\"POD_ID\"], return_counts=True)\n",
    "count_sort_ind = np.argsort(-port_count)\n",
    "print(list(zip(port_id[count_sort_ind], port_count[count_sort_ind])))\n",
    "print(len(port_id))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>From the above, AUCKLAND are the biggest export port from Singapore based on FC Cost Term</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with Auckland\n",
    "df_largest_port = df_filtered[df_filtered['POD_ID'] == 'AUCKLAND'].reset_index(drop=True)\n",
    "df_largest_port.head()\n",
    "df_largest_port.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View which is the most popular container size using numpy\n",
    "port_size, port_size_count = np.unique(df_largest_port[\"CNTR_SIZE\"], return_counts=True)\n",
    "count_sort_ind = np.argsort(-port_size_count)\n",
    "print(list(zip(port_size[count_sort_ind], port_size_count[count_sort_ind])))\n",
    "print(len(port_size))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>From the filter above, container exported are mostly size 40 and exported container sizes to Auckland is only size 40 and size 20.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with container size 40\n",
    "df_40 = df_largest_port[df_largest_port['CNTR_SIZE'] == 40].reset_index(drop=True)\n",
    "df_40.head()\n",
    "df_40.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View which is the most popular container type using numpy\n",
    "port_id, port_type_count = np.unique(df_40[\"CNTR_TYPE\"], return_counts=True)\n",
    "count_sort_ind = np.argsort(-port_type_count)\n",
    "print(list(zip(port_id[count_sort_ind], port_type_count[count_sort_ind])))\n",
    "print(len(port_id))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>For the filter above, we can see that for Auckland, container size 40, consist of container type HC, HC NOR while a few by GP and HC. HC is has the highest count for container type. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with container type HC\n",
    "df_hc = df_40[df_40['CNTR_TYPE'] == 'HC'].reset_index(drop=True)\n",
    "df_hc.head()\n",
    "df_hc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View which is the most used shipping vendor using numpy\n",
    "vendor_id, vendor_count = np.unique(df_hc[\"PARTY_NAME\"], return_counts=True)\n",
    "count_sort_ind = np.argsort(-vendor_count)\n",
    "print(list(zip(vendor_id[count_sort_ind], vendor_count[count_sort_ind])))\n",
    "print(len(vendor_id))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>From the filter above we can observe that exports for Auckland, container size 40 and type HC, the company mostly uses 'MONDIALE FREIGHT SERVICES LIMITED-AUCKLAND' quotation for all their exports.</p>\n",
    "<p>The filted dataframe only consist of 1 vendor. Hence, we do not need to filter anymore and proceed with performing modeling.</p> \n",
    "<p>After focusing in a party id, we do need ot interpolate the dates as the dates are not in series which could be a problem when performing time series models like ARIMA or Prophet.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for negative cost rate before interpolate\n",
    "for values in df_hc['RATE']:\n",
    "    if values <= 0:\n",
    "        print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"pandas\\_libs\\tslibs\\conversion.pyx\", line 530, in pandas._libs.tslibs.conversion._convert_str_to_tsobject\n",
      "  File \"pandas\\_libs\\tslibs\\parsing.pyx\", line 318, in pandas._libs.tslibs.parsing.parse_datetime_string\n",
      "  File \"c:\\Users\\Aloysius Wong\\Documents\\GitHub\\Cost-Rate-Analysis\\env\\lib\\site-packages\\dateutil\\parser\\_parser.py\", line 1368, in parse\n",
      "    return DEFAULTPARSER.parse(timestr, **kwargs)\n",
      "  File \"c:\\Users\\Aloysius Wong\\Documents\\GitHub\\Cost-Rate-Analysis\\env\\lib\\site-packages\\dateutil\\parser\\_parser.py\", line 643, in parse\n",
      "    raise ParserError(\"Unknown string format: %s\", timestr)\n",
      "dateutil.parser._parser.ParserError: Unknown string format: NZAKL\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Aloysius Wong\\Documents\\GitHub\\Cost-Rate-Analysis\\env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3460, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Aloysius Wong\\AppData\\Local\\Temp\\ipykernel_12756\\404510189.py\", line 2, in <module>\n",
      "    df_hc_filled = df_hc.groupby('POD').apply(lambda x: x.set_index('ETD_POL_D').asfreq('D').fillna(method='ffill')).reset_index()\n",
      "  File \"c:\\Users\\Aloysius Wong\\Documents\\GitHub\\Cost-Rate-Analysis\\env\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\", line 1567, in apply\n",
      "    result = self._python_apply_general(f, self._selected_obj)\n",
      "  File \"c:\\Users\\Aloysius Wong\\Documents\\GitHub\\Cost-Rate-Analysis\\env\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\", line 1629, in _python_apply_general\n",
      "    values, mutated = self.grouper.apply(f, data, self.axis)\n",
      "  File \"c:\\Users\\Aloysius Wong\\Documents\\GitHub\\Cost-Rate-Analysis\\env\\lib\\site-packages\\pandas\\core\\groupby\\ops.py\", line 839, in apply\n",
      "    res = f(group)\n",
      "  File \"C:\\Users\\Aloysius Wong\\AppData\\Local\\Temp\\ipykernel_12756\\404510189.py\", line 2, in <lambda>\n",
      "    df_hc_filled = df_hc.groupby('POD').apply(lambda x: x.set_index('ETD_POL_D').asfreq('D').fillna(method='ffill')).reset_index()\n",
      "  File \"c:\\Users\\Aloysius Wong\\Documents\\GitHub\\Cost-Rate-Analysis\\env\\lib\\site-packages\\pandas\\core\\frame.py\", line 11367, in asfreq\n",
      "    return super().asfreq(\n",
      "  File \"c:\\Users\\Aloysius Wong\\Documents\\GitHub\\Cost-Rate-Analysis\\env\\lib\\site-packages\\pandas\\core\\generic.py\", line 8235, in asfreq\n",
      "    return asfreq(\n",
      "  File \"c:\\Users\\Aloysius Wong\\Documents\\GitHub\\Cost-Rate-Analysis\\env\\lib\\site-packages\\pandas\\core\\resample.py\", line 2229, in asfreq\n",
      "    dti = date_range(obj.index.min(), obj.index.max(), freq=freq)\n",
      "  File \"c:\\Users\\Aloysius Wong\\Documents\\GitHub\\Cost-Rate-Analysis\\env\\lib\\site-packages\\pandas\\core\\indexes\\datetimes.py\", line 1125, in date_range\n",
      "    dtarr = DatetimeArray._generate_range(\n",
      "  File \"c:\\Users\\Aloysius Wong\\Documents\\GitHub\\Cost-Rate-Analysis\\env\\lib\\site-packages\\pandas\\core\\arrays\\datetimes.py\", line 361, in _generate_range\n",
      "    start = Timestamp(start)\n",
      "  File \"pandas\\_libs\\tslibs\\timestamps.pyx\", line 1698, in pandas._libs.tslibs.timestamps.Timestamp.__new__\n",
      "  File \"pandas\\_libs\\tslibs\\conversion.pyx\", line 249, in pandas._libs.tslibs.conversion.convert_to_tsobject\n",
      "  File \"pandas\\_libs\\tslibs\\conversion.pyx\", line 533, in pandas._libs.tslibs.conversion._convert_str_to_tsobject\n",
      "ValueError: could not convert string to Timestamp\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Aloysius Wong\\Documents\\GitHub\\Cost-Rate-Analysis\\env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2057, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"c:\\Users\\Aloysius Wong\\Documents\\GitHub\\Cost-Rate-Analysis\\env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1288, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"c:\\Users\\Aloysius Wong\\Documents\\GitHub\\Cost-Rate-Analysis\\env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1177, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"c:\\Users\\Aloysius Wong\\Documents\\GitHub\\Cost-Rate-Analysis\\env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1049, in structured_traceback\n",
      "    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n",
      "  File \"c:\\Users\\Aloysius Wong\\Documents\\GitHub\\Cost-Rate-Analysis\\env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 935, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"c:\\Users\\Aloysius Wong\\Documents\\GitHub\\Cost-Rate-Analysis\\env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1003, in get_records\n",
      "    lines, first = inspect.getsourcelines(etb.tb_frame)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\inspect.py\", line 1006, in getsourcelines\n",
      "    lines, lnum = findsource(object)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\inspect.py\", line 835, in findsource\n",
      "    raise OSError('could not get source code')\n",
      "OSError: could not get source code\n"
     ]
    }
   ],
   "source": [
    "# fill missing dates and values using forward fill\n",
    "df_hc_filled = df_hc.groupby('POD').apply(lambda x: x.set_index('ETD_POL_D').asfreq('D').fillna(method='ffill')).reset_index()\n",
    "\n",
    "# interpolate the missing values\n",
    "df_hc_interpolated = df_hc_filled.groupby('POD').apply(lambda x: x.interpolate(method='linear')).reset_index(drop=True)\n",
    "\n",
    "# plot the interpolated data\n",
    "df_hc_interpolated.plot(x='ETD_POL_D', y='RATE')\n",
    "plt.show()\n",
    "\n",
    "# check the info\n",
    "df_hc_interpolated.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import boxcox\n",
    "\n",
    "# # Doing symmetric distribution to determine to agg duplicated cost rate on the same date using mean or median\n",
    "# lambda_ = boxcox(interpolated_data['RATE'])\n",
    "# print(lambda_)\n",
    "# plt.hist(lambda_, bins=10)\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Negatively skewed data, it is usually recommended to use the median as the measure of central tendency instead of the mean. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # perform the aggregation and reset the index\n",
    "# df_agg = interpolated_data.groupby('POD').agg(aggregation).reset_index()\n",
    "# df_agg.head()\n",
    "# df_agg.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
